{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cancer_detection_cnn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cmichailidis/SurvivorPartB/blob/master/cancer_detection_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "F2SFVBhiNvU2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Upload your Kaggle token**"
      ]
    },
    {
      "metadata": {
        "id": "0PT2Ju3xNTgk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "koEtFhYcOMFm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Install the Kaggle API**"
      ]
    },
    {
      "metadata": {
        "id": "K4WUvifROI9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zibe2cYROzIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Copy your Kaggle token to the appropriate directory**"
      ]
    },
    {
      "metadata": {
        "id": "g59zOKCIO4ZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pW3JSd1T3WP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Download the histopathologic cancer prediction dataset**\n",
        "\n",
        "(Normally, this won't take more than three minutes to complete)"
      ]
    },
    {
      "metadata": {
        "id": "29ww9uhMPQKH",
        "colab_type": "code",
        "outputId": "793a32a4-27c9-4465-a7b3-16f5c4a1e767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c histopathologic-cancer-detection"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv.zip to /content\n",
            "\r  0% 0.00/1.33M [00:00<?, ?B/s]\n",
            "100% 1.33M/1.33M [00:00<00:00, 88.0MB/s]\n",
            "Downloading train_labels.csv.zip to /content\n",
            " 98% 5.00M/5.10M [00:00<00:00, 44.4MB/s]\n",
            "100% 5.10M/5.10M [00:00<00:00, 32.5MB/s]\n",
            "Downloading test.zip to /content\n",
            " 99% 1.30G/1.30G [00:13<00:00, 116MB/s]\n",
            "100% 1.30G/1.30G [00:13<00:00, 107MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 4.97G/4.98G [01:00<00:00, 96.5MB/s]\n",
            "100% 4.98G/4.98G [01:00<00:00, 88.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vXb0I2hCVh1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Unzip the Dataset**\n",
        "\n",
        "(This will take approximately 5 to 10 minutes...)"
      ]
    },
    {
      "metadata": {
        "id": "ppRnUFMGVlOo",
        "colab_type": "code",
        "outputId": "f25a824e-5d5f-44a9-e5f3-31419f4c25fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "kaggle.json  sample_submission.csv.zip\ttrain_labels.csv.zip\n",
            "sample_data  test.zip\t\t\ttrain.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VwP9-HEumQ-D",
        "colab_type": "code",
        "outputId": "1be16965-7587-45cf-c18e-c4c6f3429993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# Unzip the training dataset\n",
        "zipped_training_dataset = ZipFile('train.zip', 'r')\n",
        "zipped_training_dataset.extractall('train')\n",
        "zipped_training_dataset.close()\n",
        "\n",
        "# Unzip the test dataset\n",
        "zipped_test_dataset = ZipFile('test.zip', 'r')\n",
        "zipped_test_dataset.extractall('test')\n",
        "zipped_test_dataset.close()\n",
        "\n",
        "# Unzip the csv file which contains the labels of the dataset\n",
        "!unzip train_labels.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  train_labels.csv.zip\n",
            "  inflating: train_labels.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FT0FS6WQQcx3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Split the dataset and get the training and cross-validation datasets**"
      ]
    },
    {
      "metadata": {
        "id": "71kuzHF9Qdn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2y-ZbRsvm0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Split the training dataset into minibatches**\n",
        "\n",
        "(This has to be done because the dataset cannot fit on the GPU memory)"
      ]
    },
    {
      "metadata": {
        "id": "obWnxMpMvmeN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, numpy as np, cv2, random\n",
        "\n",
        "def batch_generator(input_dir, labels_file, batch_size):\n",
        "  with open(labels_file, 'r') as labels_file:\n",
        "    labels_dict = {}\n",
        "    next(labels_file)\n",
        "    os.chdir(input_dir)\n",
        "    \n",
        "    for line in labels_file:\n",
        "      image_id, label = line.split(',')\n",
        "      image_id, label = image_id + '.tif', int(label)\n",
        "      labels_dict[image_id] = label\n",
        "      \n",
        "    image_ids = np.array(labels_dict.keys())\n",
        "    m = len(os.listdir(input_dir))\n",
        "    N = m//batch_size\n",
        "    \n",
        "    if N==0:\n",
        "      raise ValueError('Batch size is bigger than the dataset!')\n",
        "    \n",
        "    while True:\n",
        "      np.random.shuffle(image_ids)\n",
        "      batches = [image_ids[i*batch_size:(i+1)*batch_size] for i in range(N)] \\\n",
        "      + [image_ids[N*batch_size:]]\n",
        "      \n",
        "      for minibatch in batches:\n",
        "        x_train, y_train = [], []\n",
        "        \n",
        "        for image in minibatch:\n",
        "          x_train.append(cv2.imread(image)/256.0)\n",
        "          y_train.append(labels_dict[image])\n",
        "        \n",
        "        yield np.array(x_train), np.array(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0LshnCXrERpM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Define the architecture of your prediction model**"
      ]
    },
    {
      "metadata": {
        "id": "kEwFrMA0_hRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout\n",
        "from keras.layers import MaxPooling2D as Pool2D\n",
        "from keras.layers import Convolution2D as Conv2D\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.optimizers import SGD, Adam, Nadam, RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_oWbwXAoNOtV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "model = Sequential([\n",
        "    Conv2D(filtes=4,\n",
        "           kernel_size=(3,3),\n",
        "           activation='relu',\n",
        "           data_format='channels_last',\n",
        "           input_shape=(h,w,d)),\n",
        "    \n",
        "    Pool2D(pool_size=(2,2)),\n",
        "    \n",
        "    Conv2D(filters=8,\n",
        "           kernel_size=(3,3),\n",
        "           activation='relu'),\n",
        "    \n",
        "    Pool2D(pool_size=(2,2)),\n",
        "    \n",
        "    Conv2D(filters=16,\n",
        "           kernel_size=(3,3),\n",
        "           activation='relu'),\n",
        "    \n",
        "    Pool2D(pool_size=(2,2)),\n",
        "    \n",
        "    Conv2D(filters=32,\n",
        "           kernel_size=(3,3),\n",
        "           activation='relu'),\n",
        "    \n",
        "    Pool2D(pool_size=(2,2)),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    Dense(units=128,\n",
        "          activation='relu'),\n",
        "    \n",
        "    Dropout(0.4),\n",
        "    \n",
        "    Dense(units=64,\n",
        "          activation='relu'),\n",
        "    \n",
        "    Dropout(0.3),\n",
        "    \n",
        "    Dense(units=1,\n",
        "          activation='sigmoid')   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Tv9Mp9ROghm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r3l4oGFVPQuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train the neural network**"
      ]
    },
    {
      "metadata": {
        "id": "MBTjC0CkOiUG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "model.compile()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vqC52nHOj2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "model.fit_generator()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qGSzpEZTPhgR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test your CNN on the cross-validation dataset to make sure that you have not overfit the dataset**"
      ]
    },
    {
      "metadata": {
        "id": "gtDANxKgOlGu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "model.evaluate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3_pt2vfMPQmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Test your CNN on the test dataset and store your predictions on a csv file**"
      ]
    },
    {
      "metadata": {
        "id": "2QCoY-ydOyqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "with open('output.csv') as output_file:\n",
        "  predictions = model.predict(x_test)\n",
        "  \n",
        "  output.write('Id,Label\\n')\n",
        "  \n",
        "  for prediction in predictions:\n",
        "    output.write('{:s},{:d}\\n'.format(prediction, labels_dict[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VIXUIgzUaHW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}